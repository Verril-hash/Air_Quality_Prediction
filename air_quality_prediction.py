"""
================================================================================
ğŸŒ AIR QUALITY PREDICTION & FORECASTING SYSTEM
================================================================================
A comprehensive PySpark project demonstrating:
  âœ“ SparkContext & Driver Program concepts
  âœ“ RDD (Resilient Distributed Dataset) operations
  âœ“ DataFrame API and Spark SQL
  âœ“ Machine Learning with MLlib
  âœ“ Transformations and Actions
  âœ“ Partitioning and Caching strategies
  âœ“ AQI Prediction and Forecasting

Author: Air Quality Analytics Team
Dataset: Global Air Quality Dataset (87k+ hourly readings)
================================================================================

SPARK ARCHITECTURE CONCEPTS DEMONSTRATED:
-----------------------------------------
1. DRIVER PROGRAM: This script runs on the driver node, coordinates execution
2. SPARKCONTEXT (SC): Entry point for Spark functionality (low-level RDD API)
3. SPARKSESSION: Unified entry point (combines SQLContext, HiveContext, SparkContext)
4. RDD: Immutable distributed collection with transformations & actions
5. DATAFRAME: Distributed collection organized into named columns (higher-level API)
6. TRANSFORMATIONS: Lazy operations (map, filter, reduceByKey, groupByKey, etc.)
7. ACTIONS: Trigger computation (collect, count, reduce, take, saveAsTextFile)
8. EXECUTORS: Worker processes that run computations and store data
9. PARTITIONS: Logical division of data across cluster
10. DAG: Directed Acyclic Graph of operations
================================================================================
"""

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                    SECTION 1: IMPORTS & CONFIGURATION                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

import os
import sys
import math
from datetime import datetime, timedelta
from operator import add

# Set PySpark Python path BEFORE importing pyspark
# This fixes the "Python was not found" error on Windows
python_path = sys.executable
os.environ['PYSPARK_PYTHON'] = python_path
os.environ['PYSPARK_DRIVER_PYTHON'] = python_path

# Core Spark imports
from pyspark import SparkConf, SparkContext
from pyspark.sql import SparkSession, Window, Row
from pyspark.sql.functions import (
    col, mean, desc, lag, when, to_timestamp, hour, dayofweek, month, year,
    isnan, isnull, count, lit, avg, max as spark_max, min as spark_min,
    stddev, round as spark_round, expr, row_number, udf, monotonically_increasing_id,
    date_add, current_timestamp, greatest, sum as spark_sum, broadcast
)
from pyspark.sql.types import (
    DoubleType, IntegerType, StringType, StructType, StructField, 
    TimestampType, FloatType, LongType, ArrayType
)

# RDD specific imports
from pyspark.rdd import RDD

# ML imports
from pyspark.ml import Pipeline
from pyspark.ml.feature import (
    VectorAssembler, StandardScaler, StringIndexer, OneHotEncoder,
    Imputer, MinMaxScaler
)
from pyspark.ml.regression import RandomForestRegressor, GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

# Broadcast and Accumulator imports
from pyspark import Accumulator, AccumulatorParam

# Configuration Constants
APP_NAME = "AirQualityPrediction"
CSV_PATH = "air_quality.csv"
PREDICTIONS_OUTPUT = "aqi_predictions_output.csv"
HIGH_RISK_OUTPUT = "aqi_high_risk_zones.csv"
RDD_OUTPUT = "aqi_rdd_analysis.csv"
NUM_PARTITIONS = 20
RANDOM_SEED = 42
TRAIN_RATIO = 0.8
TEST_RATIO = 0.2

print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        ğŸŒ¬ï¸  AIR QUALITY PREDICTION & FORECASTING SYSTEM  ğŸŒ¬ï¸                  â•‘
â•‘                     Powered by Apache Spark MLlib                            â•‘
â•‘                                                                              â•‘
â•‘     Demonstrating: SparkContext, RDD, Driver, Transformations, Actions      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                     SECTION 2: SPARK SESSION & CONTEXT                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("="*80)
print("âš™ï¸  SPARK INITIALIZATION - DRIVER PROGRAM STARTS")
print("="*80)

print("""
ğŸ“š SPARK ARCHITECTURE OVERVIEW:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DRIVER PROGRAM (This Script)                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  SparkContext (sc) - Low-level API for RDD operations               â”‚   â”‚
â”‚  â”‚  SparkSession (spark) - Unified entry point for DataFrame/SQL       â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                    â†“                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                      â”‚
â”‚  â”‚  Executor 1  â”‚  â”‚  Executor 2  â”‚  â”‚  Executor N  â”‚   (Worker Nodes)     â”‚
â”‚  â”‚  Partition 1 â”‚  â”‚  Partition 2 â”‚  â”‚  Partition N â”‚                      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

print("\nğŸš€ Initializing SparkSession and SparkContext...")

try:
    # ============================================================
    # SPARK CONFIGURATION - Sets cluster/application parameters
    # ============================================================
    conf = SparkConf() \
        .setAppName(APP_NAME) \
        .set("spark.sql.shuffle.partitions", str(NUM_PARTITIONS)) \
        .set("spark.driver.memory", "4g") \
        .set("spark.executor.memory", "2g") \
        .set("spark.sql.adaptive.enabled", "true") \
        .set("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
    
    # ============================================================
    # SPARKSESSION - Unified entry point (DataFrame, SQL, Streaming)
    # ============================================================
    spark = SparkSession.builder \
        .config(conf=conf) \
        .getOrCreate()
    
    # ============================================================
    # SPARKCONTEXT - Low-level API for RDD operations
    # The SparkContext is the heart of any Spark application
    # ============================================================
    sc = spark.sparkContext
    
    # Set log level to reduce verbosity
    sc.setLogLevel("WARN")
    
    print("\n" + "="*80)
    print("âœ… SPARK INITIALIZATION SUCCESSFUL")
    print("="*80)
    print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  DRIVER INFORMATION                                                     â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  ğŸ“Œ Application Name:     {sc.appName:<40} â”‚
    â”‚  ğŸ“Œ Spark Version:        {spark.version:<40} â”‚
    â”‚  ğŸ“Œ Master URL:           {sc.master:<40} â”‚
    â”‚  ğŸ“Œ Python Version:       {sys.version.split()[0]:<40} â”‚
    â”‚  ğŸ“Œ Default Parallelism:  {sc.defaultParallelism:<40} â”‚
    â”‚  ğŸ“Œ Application ID:       {sc.applicationId:<40} â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    print("""
    ğŸ“š KEY CONCEPTS:
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  â€¢ SparkContext (sc): Entry point for RDD programming                   â”‚
    â”‚  â€¢ SparkSession (spark): Unified entry for DataFrame, SQL, Streaming   â”‚
    â”‚  â€¢ Driver: Runs main program, creates SparkContext, divides into tasks â”‚
    â”‚  â€¢ Executor: Runs tasks assigned by driver on worker nodes              â”‚
    â”‚  â€¢ Default Parallelism: Number of partitions for RDD operations         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
except Exception as e:
    print(f"âŒ Failed to create Spark Session: {e}")
    sys.exit(1)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                      SECTION 3: DATA LOADING & CACHING                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ“¥ SECTION 3: DATA LOADING & RDD FUNDAMENTALS")
print("="*80)

print("""
ğŸ“š RDD (Resilient Distributed Dataset) CONCEPTS:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RDD Properties:                                                            â”‚
â”‚  1. IMMUTABLE: Cannot be changed once created                               â”‚
â”‚  2. DISTRIBUTED: Data split across cluster nodes (partitions)              â”‚
â”‚  3. RESILIENT: Can recover from node failures using lineage                â”‚
â”‚  4. LAZY EVALUATION: Transformations not executed until action called      â”‚
â”‚                                                                             â”‚
â”‚  Two types of operations:                                                   â”‚
â”‚  â€¢ TRANSFORMATIONS (Lazy): map, filter, flatMap, reduceByKey, groupByKey   â”‚
â”‚  â€¢ ACTIONS (Trigger execution): collect, count, reduce, take, first        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

try:
    # ============================================================
    # LOAD CSV AS DATAFRAME
    # ============================================================
    print("ğŸ“‚ Loading CSV file into DataFrame...")
    
    df_raw = spark.read.csv(
        CSV_PATH,
        header=True,
        inferSchema=True
    )
    
    # Repartition for scalability
    df_raw = df_raw.repartition(NUM_PARTITIONS)
    
    # ============================================================
    # CONVERT DATAFRAME TO RDD - Demonstrates RDD operations
    # ============================================================
    print("\nğŸ”„ Converting DataFrame to RDD for low-level operations...")
    
    # Get the underlying RDD from DataFrame
    raw_rdd = df_raw.rdd
    
    print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  RDD INFORMATION                                                        â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  ğŸ“Š Number of Partitions: {raw_rdd.getNumPartitions():<40} â”‚
    â”‚  ğŸ“Š RDD ID: {raw_rdd.id():<50} â”‚
    â”‚  ğŸ“Š Is Cached: {str(raw_rdd.is_cached):<47} â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    """)
    
    # ============================================================
    # CACHE/PERSIST - Stores RDD in memory for faster access
    # ============================================================
    print("\nğŸ’¾ CACHING DATA (Storing in memory for faster access)...")
    
    # Cache the DataFrame (uses MEMORY_AND_DISK storage level by default)
    df_raw = df_raw.cache()
    
    # Force materialization with COUNT action
    # Actions trigger the execution of all preceding transformations
    record_count = df_raw.count()  # ACTION - triggers computation
    
    print(f"""
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  âœ… DATA LOADING COMPLETE                                               â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  ğŸ“Š Total Records Loaded: {record_count:<40,} â”‚
    â”‚  ğŸ“Š Number of Partitions: {df_raw.rdd.getNumPartitions():<40} â”‚
    â”‚  ğŸ“Š Data is Cached: {str(df_raw.is_cached):<47} â”‚
    â”‚  ğŸ“Š Number of Columns: {len(df_raw.columns):<45} â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    
    ğŸ’¡ Note: The count() action forced Spark to execute all lazy 
       transformations and load the data into memory.
    """)
    
except Exception as e:
    print(f"âŒ Failed to load data: {e}")
    spark.stop()
    sys.exit(1)


# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                    SECTION 3.5: RDD OPERATIONS DEMONSTRATION               â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ”§ SECTION 3.5: RDD TRANSFORMATIONS & ACTIONS CONCEPTS")
print("="*80)

print("""
ğŸ“š RDD TRANSFORMATIONS (Lazy - return new RDD):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â€¢ map(func)        : Apply function to each element                        â”‚
â”‚  â€¢ filter(func)     : Select elements where function returns true           â”‚
â”‚  â€¢ flatMap(func)    : Map then flatten results                              â”‚
â”‚  â€¢ reduceByKey(func): Aggregate values by key                               â”‚
â”‚  â€¢ groupByKey()     : Group values by key                                   â”‚
â”‚  â€¢ mapPartitions()  : Apply function to each partition                      â”‚
â”‚  â€¢ distinct()       : Remove duplicates                                     â”‚
â”‚  â€¢ union()          : Combine two RDDs                                      â”‚
â”‚  â€¢ join()           : Join two RDDs by key                                  â”‚
â”‚  â€¢ sortByKey()      : Sort RDD by key                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“š RDD ACTIONS (Trigger computation - return values):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â€¢ collect()        : Return all elements to driver                         â”‚
â”‚  â€¢ count()          : Return number of elements                             â”‚
â”‚  â€¢ first()          : Return first element                                  â”‚
â”‚  â€¢ take(n)          : Return first n elements                               â”‚
â”‚  â€¢ reduce(func)     : Aggregate elements using function                     â”‚
â”‚  â€¢ foreach(func)    : Apply function to each element                        â”‚
â”‚  â€¢ saveAsTextFile() : Save to text file                                     â”‚
â”‚  â€¢ countByKey()     : Count elements per key                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ“š NOTE: DataFrame operations are built on top of RDDs!
   Every DataFrame operation translates to RDD transformations internally.
   DataFrame provides: Catalyst optimizer + Tungsten engine for better performance
""")

# ============================================================
# DEMONSTRATION USING DATAFRAME API (Internally uses RDD)
# The DataFrame API provides a higher-level abstraction over RDDs
# ============================================================

print("ğŸ”¬ Demonstrating RDD Concepts via DataFrame Operations...\n")

# Create temporary view for SQL operations
df_raw.createOrReplaceTempView("air_quality_rdd_demo")

# Get the underlying RDD from DataFrame
data_rdd = df_raw.rdd

# ------------------------------------------------------------
# RDD BASIC INFO - getNumPartitions(), id()
# ------------------------------------------------------------
print("1ï¸âƒ£  RDD BASICS - Partitions and Structure:")
print("-"*60)
print(f"   Number of Partitions: {data_rdd.getNumPartitions()}")
print(f"   RDD ID: {data_rdd.id()}")
print(f"   RDD Name: {data_rdd.name()}")
print("""
   ğŸ“š Partitions divide data across cluster for parallel processing.
      Each partition is processed by a separate task on an executor.
""")

# ------------------------------------------------------------
# TRANSFORMATION vs ACTION concept
# ------------------------------------------------------------
print("\n2ï¸âƒ£  TRANSFORMATION vs ACTION:")
print("-"*60)
print("""
   TRANSFORMATIONS (Lazy - just build the execution plan):
   â€¢ df.select("pm25", "pm10")     â†’ equivalent to RDD.map()
   â€¢ df.filter(col("pm25") > 50)   â†’ equivalent to RDD.filter()
   â€¢ df.distinct()                  â†’ equivalent to RDD.distinct()
   â€¢ df.groupBy("zone")            â†’ equivalent to RDD.groupByKey()
   
   ACTIONS (Trigger computation - return results to driver):
   â€¢ df.count()                    â†’ equivalent to RDD.count()
   â€¢ df.collect()                  â†’ equivalent to RDD.collect()
   â€¢ df.show()                     â†’ displays data (triggers computation)
   â€¢ df.first()                    â†’ equivalent to RDD.first()
""")

# ------------------------------------------------------------
# COUNT - Demonstrating an ACTION
# ------------------------------------------------------------
print("\n3ï¸âƒ£  COUNT ACTION - Triggers execution of DAG:")
print("-"*60)
# Using DataFrame count (internally uses RDD operations)
df_count = df_raw.count()
print(f"   df.count() = {df_count:,} records")
print("   â†³ Equivalent RDD code: rdd.count()")

# ------------------------------------------------------------
# FILTER TRANSFORMATION
# ------------------------------------------------------------
print("\n4ï¸âƒ£  FILTER TRANSFORMATION - Select high PM2.5:")
print("-"*60)
high_pm25_df = df_raw.filter(col("pm25") > 50)
high_pm25_count = high_pm25_df.count()
print(f"   df.filter(col('pm25') > 50).count() = {high_pm25_count:,}")
print("   â†³ Equivalent RDD code: rdd.filter(lambda x: x['pm25'] > 50).count()")

# ------------------------------------------------------------
# MAP (SELECT) TRANSFORMATION
# ------------------------------------------------------------
print("\n5ï¸âƒ£  MAP TRANSFORMATION (via SELECT):")
print("-"*60)
print("   df.select('pm25', 'pm10') â†’ maps each row to subset of columns")
print("   â†³ Equivalent RDD code: rdd.map(lambda x: (x['pm25'], x['pm10']))")
df_raw.select("pm25", "pm10").show(5)

# ------------------------------------------------------------
# GROUPBY (equivalent to groupByKey/reduceByKey)
# ------------------------------------------------------------
print("\n6ï¸âƒ£  REDUCEBYKEY via GROUPBY + AGGREGATE:")
print("-"*60)
print("   df.groupBy('hour').agg(avg('pm25'), sum('pm25'))")
print("   â†³ Equivalent RDD code: rdd.map((hour, pm25)).reduceByKey(+)")
hourly_stats = df_raw.withColumn("hour", hour(to_timestamp("datetime"))) \
    .groupBy("hour") \
    .agg(
        spark_round(avg("pm25"), 2).alias("avg_pm25"),
        spark_round(spark_sum("pm25"), 2).alias("total_pm25"),
        count("*").alias("count")
    ) \
    .orderBy("hour")
hourly_stats.show(5)

# ------------------------------------------------------------
# DISTINCT TRANSFORMATION
# ------------------------------------------------------------
print("\n7ï¸âƒ£  DISTINCT TRANSFORMATION:")
print("-"*60)
print("   df.select('datetime').distinct() â†’ unique values")
print("   â†³ Equivalent RDD code: rdd.map(datetime).distinct()")
unique_dates = df_raw.select(expr("DATE(TO_TIMESTAMP(datetime))").alias("date")).distinct().count()
print(f"   Unique dates in dataset: {unique_dates}")

# ------------------------------------------------------------
# AGGREGATE (REDUCE equivalent)
# ------------------------------------------------------------
print("\n8ï¸âƒ£  REDUCE via AGGREGATE:")
print("-"*60)
totals = df_raw.agg(
    spark_sum("pm25").alias("total_pm25"),
    spark_sum("pm10").alias("total_pm10"),
    avg("pm25").alias("avg_pm25")
).collect()[0]
print(f"   Total PM2.5: {totals['total_pm25']:,.2f}")
print(f"   Total PM10: {totals['total_pm10']:,.2f}")
print(f"   Avg PM2.5: {totals['avg_pm25']:.2f}")
print("   â†³ Equivalent RDD code: rdd.map(pm25).reduce(lambda a,b: a+b)")

# ------------------------------------------------------------
# ACCUMULATOR - Shared variable for aggregation
# ------------------------------------------------------------
print("\n9ï¸âƒ£  ACCUMULATOR - Shared counter variable:")
print("-"*60)

# Create accumulator (shared variable updated by tasks)
record_counter = sc.accumulator(0)
print(f"   Created accumulator: sc.accumulator(0)")
print(f"   Initial value: {record_counter.value}")
print("""
   ğŸ“š Accumulators are:
      â€¢ Write-only from workers, read-only from driver
      â€¢ Used for counters and sums across distributed tasks
      â€¢ Updated atomically to prevent race conditions
""")

# ------------------------------------------------------------
# BROADCAST VARIABLE - Shared read-only data
# ------------------------------------------------------------
print("\nğŸ”Ÿ  BROADCAST VARIABLE - Share data to all nodes:")
print("-"*60)

# Create a lookup table (broadcast to all nodes)
aqi_categories = {
    'Good': (0, 50),
    'Moderate': (51, 100),
    'Unhealthy_Sensitive': (101, 150),
    'Unhealthy': (151, 200),
    'Very_Unhealthy': (201, 300),
    'Hazardous': (301, 500)
}

broadcast_categories = sc.broadcast(aqi_categories)
print(f"   Created broadcast variable: sc.broadcast(aqi_categories)")
print(f"   Categories shared: {list(broadcast_categories.value.keys())}")
print("""
   ğŸ“š Broadcast Variables are:
      â€¢ Read-only shared data sent to all worker nodes
      â€¢ Cached on each executor (not sent per task)
      â€¢ Efficient for sharing large lookup tables
""")

# ------------------------------------------------------------
# SPARK SQL - Higher level abstraction
# ------------------------------------------------------------
print("\n1ï¸âƒ£1ï¸âƒ£  SPARK SQL - Query engine over RDDs:")
print("-"*60)
print("   SQL queries are translated to RDD operations internally")

spark.sql("""
    SELECT 
        ROUND(AVG(pm25), 2) as avg_pm25,
        ROUND(MAX(pm25), 2) as max_pm25,
        COUNT(*) as total_records
    FROM air_quality_rdd_demo
    WHERE pm25 IS NOT NULL
""").show()

# ------------------------------------------------------------
# RDD LINEAGE
# ------------------------------------------------------------
print("\n1ï¸âƒ£2ï¸âƒ£  RDD LINEAGE - Tracking transformations:")
print("-"*60)
print("""
   ğŸ“š RDD Lineage (also called DAG):
      â€¢ Records all transformations applied to create an RDD
      â€¢ Enables fault tolerance - can recompute lost partitions
      â€¢ Visible in Spark UI as job execution graph
      
   Example lineage for our data:
   TextFile â†’ Parse CSV â†’ Repartition â†’ Filter â†’ Map â†’ Cache
""")

print("\n" + "="*80)
print("âœ… RDD CONCEPTS DEMONSTRATION COMPLETE")
print("="*80)
print("""
ğŸ“Š SUMMARY OF SPARK CONCEPTS COVERED:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  RDD OPERATIONS (shown via DataFrame equivalents):                          â”‚
â”‚    âœ“ map()           â†’ df.select() - transform columns                     â”‚
â”‚    âœ“ filter()        â†’ df.filter() - filter rows                           â”‚
â”‚    âœ“ reduceByKey()   â†’ df.groupBy().agg() - aggregate by key               â”‚
â”‚    âœ“ distinct()      â†’ df.distinct() - remove duplicates                   â”‚
â”‚    âœ“ count()         â†’ df.count() - count records                          â”‚
â”‚    âœ“ collect()       â†’ df.collect() - retrieve to driver                   â”‚
â”‚                                                                             â”‚
â”‚  SHARED VARIABLES:                                                          â”‚
â”‚    âœ“ Accumulator     - Write-only counter across tasks                     â”‚
â”‚    âœ“ Broadcast       - Read-only data shared to all nodes                  â”‚
â”‚                                                                             â”‚
â”‚  CORE CONCEPTS:                                                             â”‚
â”‚    âœ“ Partitions      - Data divided for parallel processing                â”‚
â”‚    âœ“ Lazy Evaluation - Transformations not executed until action           â”‚
â”‚    âœ“ Lineage/DAG     - Track transformations for fault tolerance           â”‚
â”‚    âœ“ Spark SQL       - SQL queries over distributed data                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                   SECTION 4: EXPLORATORY DATA ANALYSIS                     â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ” SECTION 4: EXPLORATORY DATA ANALYSIS + SPARK SQL")
print("="*80)

print("""
ğŸ“š SPARK SQL & DATAFRAME API:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DataFrame = RDD + Schema (column names & types)                            â”‚
â”‚                                                                             â”‚
â”‚  Key Advantages over RDD:                                                   â”‚
â”‚  1. Catalyst Optimizer - Automatic query optimization                       â”‚
â”‚  2. Tungsten Engine - Memory management & code generation                   â”‚
â”‚  3. Schema - Named columns with data types                                  â”‚
â”‚  4. SQL Interface - Query data using SQL syntax                             â”‚
â”‚                                                                             â”‚
â”‚  Conversion:                                                                â”‚
â”‚  â€¢ DataFrame â†’ RDD: df.rdd                                                  â”‚
â”‚  â€¢ RDD â†’ DataFrame: spark.createDataFrame(rdd, schema)                      â”‚
â”‚  â€¢ RDD â†’ DataFrame: rdd.toDF()                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# 4.1 Schema Analysis
print("\nğŸ“‹ Dataset Schema (DataFrame Metadata):")
print("-"*50)
df_raw.printSchema()

# 4.2 Register DataFrame as Temporary View for SQL queries
print("\nğŸ“Š Registering DataFrame as SQL Temporary View...")
df_raw.createOrReplaceTempView("air_quality")
print("   âœ… View 'air_quality' created - can now use SQL queries!")

# 4.3 Demonstrate Spark SQL
print("\n" + "="*80)
print("ğŸ” SPARK SQL DEMONSTRATION")
print("="*80)

print("\n1ï¸âƒ£  SQL Query - Basic Statistics:")
print("-"*60)

sql_stats = spark.sql("""
    SELECT 
        COUNT(*) as total_records,
        ROUND(AVG(pm25), 2) as avg_pm25,
        ROUND(AVG(pm10), 2) as avg_pm10,
        ROUND(MAX(pm25), 2) as max_pm25,
        ROUND(MIN(pm25), 2) as min_pm25
    FROM air_quality
    WHERE pm25 IS NOT NULL
""")
sql_stats.show()

print("\n2ï¸âƒ£  SQL Query - Hourly Pollution Pattern:")
print("-"*60)

sql_hourly = spark.sql("""
    SELECT 
        HOUR(TO_TIMESTAMP(datetime)) as hour,
        ROUND(AVG(pm25), 2) as avg_pm25,
        ROUND(AVG(pm10), 2) as avg_pm10,
        COUNT(*) as record_count
    FROM air_quality
    WHERE pm25 IS NOT NULL
    GROUP BY HOUR(TO_TIMESTAMP(datetime))
    ORDER BY hour
""")
sql_hourly.show(24)

print("\n3ï¸âƒ£  SQL Query with Window Functions:")
print("-"*60)

sql_window = spark.sql("""
    SELECT 
        datetime,
        pm25,
        ROUND(AVG(pm25) OVER (
            ORDER BY datetime 
            ROWS BETWEEN 3 PRECEDING AND CURRENT ROW
        ), 2) as rolling_avg_pm25
    FROM air_quality
    WHERE pm25 IS NOT NULL
    LIMIT 10
""")
sql_window.show()

# 4.4 Column Names Check
print("\nğŸ“‹ Available Columns:")
print(f"   {df_raw.columns}")

# 4.5 Statistical Summary using DataFrame API
print("\nğŸ“Š Statistical Summary (DataFrame API):")
print("-"*50)
df_raw.describe().show()

# 4.6 Missing Value Analysis
print("\nğŸ” Missing Value Analysis:")
print("-"*50)

def analyze_missing_values(df):
    """Analyze and display missing values for each column."""
    missing_stats = []
    total_rows = df.count()
    
    # Only check numeric columns for NaN (isnan doesn't work on timestamps)
    numeric_columns = [f.name for f in df.schema.fields 
                       if str(f.dataType) in ['DoubleType()', 'FloatType()', 'IntegerType()', 'LongType()']]
    
    for column in df.columns:
        # For numeric columns, check both null and NaN
        if column in numeric_columns:
            null_count = df.filter(col(column).isNull() | isnan(col(column))).count()
            # Count special missing values (-999, -1, etc.)
            special_missing = df.filter(col(column).isin([-999, -999.0, -1, -1.0])).count()
        else:
            # For non-numeric columns, only check null
            null_count = df.filter(col(column).isNull()).count()
            special_missing = 0
            
        total_missing = null_count + special_missing
        missing_pct = (total_missing / total_rows) * 100
        
        if total_missing > 0:
            missing_stats.append({
                'column': column,
                'null_count': null_count,
                'special_missing': special_missing,
                'total_missing': total_missing,
                'missing_pct': missing_pct
            })
    
    if missing_stats:
        for stat in missing_stats:
            print(f"   âš ï¸  {stat['column']}: {stat['total_missing']:,} missing ({stat['missing_pct']:.2f}%)")
    else:
        print("   âœ… No missing values detected!")
    
    return missing_stats

missing_analysis = analyze_missing_values(df_raw)

# 4.7 Data Quality Check
print("\nğŸ“Š Data Range Analysis:")
print("-"*50)

numeric_cols = ['pm25', 'pm10', 'no2', 'co', 'so2', 'o3', 'temp', 'rh', 'wind', 'rain']
for col_name in numeric_cols:
    if col_name in df_raw.columns:
        stats = df_raw.agg(
            spark_min(col_name).alias('min'),
            spark_max(col_name).alias('max'),
            mean(col_name).alias('mean')
        ).collect()[0]
        print(f"   {col_name:10}: Min={stats['min']:.2f}, Max={stats['max']:.2f}, Mean={stats['mean']:.2f}")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                       SECTION 5: DATA PREPROCESSING                        â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ”§ DATA PREPROCESSING")
print("="*80)

# 5.1 Handle Missing Values
print("\nğŸ§¹ Handling Missing Values...")

# Replace special missing values with null
df_clean = df_raw
for col_name in numeric_cols:
    if col_name in df_clean.columns:
        df_clean = df_clean.withColumn(
            col_name,
            when((col(col_name) == -999) | (col(col_name) == -1) | isnan(col(col_name)), None)
            .otherwise(col(col_name))
        )

# Impute missing numeric values with column means
try:
    # Calculate means for imputation
    means = df_clean.select([mean(c).alias(c) for c in numeric_cols if c in df_clean.columns]).collect()[0]
    
    for col_name in numeric_cols:
        if col_name in df_clean.columns:
            mean_val = means[col_name]
            if mean_val is not None:
                df_clean = df_clean.fillna({col_name: mean_val})
    
    print("   âœ… Missing values imputed with column means")
except Exception as e:
    print(f"   âš ï¸  Imputation warning: {e}")

# 5.2 Filter Valid Rows
print("\nğŸ” Filtering Valid Records...")
original_count = df_clean.count()

# Filter out rows with negative pollutant values (invalid readings)
df_clean = df_clean.filter(
    (col('pm25') >= 0) & 
    (col('pm10') >= 0) & 
    (col('no2') >= 0) &
    (col('co') >= 0) &
    (col('so2') >= 0) &
    (col('o3') >= 0)
)

filtered_count = df_clean.count()
print(f"   ğŸ“Š Records before filtering: {original_count:,}")
print(f"   ğŸ“Š Records after filtering: {filtered_count:,}")
print(f"   ğŸ“Š Removed: {original_count - filtered_count:,} invalid records")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                      SECTION 6: AQI CALCULATION (EPA)                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ§® AQI CALCULATION (EPA Standard)")
print("="*80)

# EPA AQI Breakpoints for each pollutant
# Reference: https://www.airnow.gov/aqi/aqi-basics/

def calculate_sub_index(concentration, breakpoints):
    """
    Calculate AQI sub-index for a pollutant based on EPA breakpoints.
    breakpoints: list of tuples (C_lo, C_hi, I_lo, I_hi)
    """
    for C_lo, C_hi, I_lo, I_hi in breakpoints:
        if C_lo <= concentration <= C_hi:
            return ((I_hi - I_lo) / (C_hi - C_lo)) * (concentration - C_lo) + I_lo
    return 500  # Hazardous if beyond scale

# Define EPA AQI calculation using Spark SQL expressions
# PM2.5 breakpoints (24-hour, Âµg/mÂ³)
print("\nğŸ“ Calculating AQI sub-indices for each pollutant...")

# PM2.5 Sub-Index (simplified EPA calculation)
df_aqi = df_clean.withColumn(
    "aqi_pm25",
    when(col("pm25") <= 12.0, (50/12.0) * col("pm25"))
    .when((col("pm25") > 12.0) & (col("pm25") <= 35.4), 50 + ((100-50)/(35.4-12.0)) * (col("pm25") - 12.0))
    .when((col("pm25") > 35.4) & (col("pm25") <= 55.4), 100 + ((150-100)/(55.4-35.4)) * (col("pm25") - 35.4))
    .when((col("pm25") > 55.4) & (col("pm25") <= 150.4), 150 + ((200-150)/(150.4-55.4)) * (col("pm25") - 55.4))
    .when((col("pm25") > 150.4) & (col("pm25") <= 250.4), 200 + ((300-200)/(250.4-150.4)) * (col("pm25") - 150.4))
    .when((col("pm25") > 250.4) & (col("pm25") <= 350.4), 300 + ((400-300)/(350.4-250.4)) * (col("pm25") - 250.4))
    .when(col("pm25") > 350.4, 400 + ((500-400)/(500.4-350.4)) * (col("pm25") - 350.4))
    .otherwise(0)
)

# PM10 Sub-Index (24-hour, Âµg/mÂ³)
df_aqi = df_aqi.withColumn(
    "aqi_pm10",
    when(col("pm10") <= 54, (50/54) * col("pm10"))
    .when((col("pm10") > 54) & (col("pm10") <= 154), 50 + ((100-50)/(154-54)) * (col("pm10") - 54))
    .when((col("pm10") > 154) & (col("pm10") <= 254), 100 + ((150-100)/(254-154)) * (col("pm10") - 154))
    .when((col("pm10") > 254) & (col("pm10") <= 354), 150 + ((200-150)/(354-254)) * (col("pm10") - 254))
    .when((col("pm10") > 354) & (col("pm10") <= 424), 200 + ((300-200)/(424-354)) * (col("pm10") - 354))
    .when(col("pm10") > 424, 300 + ((500-300)/(604-424)) * (col("pm10") - 424))
    .otherwise(0)
)

# O3 Sub-Index (8-hour, ppb -> ppm conversion applied)
df_aqi = df_aqi.withColumn(
    "aqi_o3",
    when(col("o3") <= 54, (50/54) * col("o3"))
    .when((col("o3") > 54) & (col("o3") <= 70), 50 + ((100-50)/(70-54)) * (col("o3") - 54))
    .when((col("o3") > 70) & (col("o3") <= 85), 100 + ((150-100)/(85-70)) * (col("o3") - 70))
    .when((col("o3") > 85) & (col("o3") <= 105), 150 + ((200-150)/(105-85)) * (col("o3") - 85))
    .when(col("o3") > 105, 200 + ((300-200)/(200-105)) * (col("o3") - 105))
    .otherwise(0)
)

# NO2 Sub-Index
df_aqi = df_aqi.withColumn(
    "aqi_no2",
    when(col("no2") <= 53, (50/53) * col("no2"))
    .when((col("no2") > 53) & (col("no2") <= 100), 50 + ((100-50)/(100-53)) * (col("no2") - 53))
    .when((col("no2") > 100) & (col("no2") <= 360), 100 + ((150-100)/(360-100)) * (col("no2") - 100))
    .when(col("no2") > 360, 150 + ((200-150)/(649-360)) * (col("no2") - 360))
    .otherwise(0)
)

# SO2 Sub-Index
df_aqi = df_aqi.withColumn(
    "aqi_so2",
    when(col("so2") <= 35, (50/35) * col("so2"))
    .when((col("so2") > 35) & (col("so2") <= 75), 50 + ((100-50)/(75-35)) * (col("so2") - 35))
    .when((col("so2") > 75) & (col("so2") <= 185), 100 + ((150-100)/(185-75)) * (col("so2") - 75))
    .when(col("so2") > 185, 150 + ((200-150)/(304-185)) * (col("so2") - 185))
    .otherwise(0)
)

# CO Sub-Index (mg/mÂ³)
df_aqi = df_aqi.withColumn(
    "aqi_co",
    when(col("co") <= 4.4, (50/4.4) * col("co"))
    .when((col("co") > 4.4) & (col("co") <= 9.4), 50 + ((100-50)/(9.4-4.4)) * (col("co") - 4.4))
    .when((col("co") > 9.4) & (col("co") <= 12.4), 100 + ((150-100)/(12.4-9.4)) * (col("co") - 9.4))
    .when(col("co") > 12.4, 150 + ((200-150)/(15.4-12.4)) * (col("co") - 12.4))
    .otherwise(0)
)

# Calculate Overall AQI as the maximum of all sub-indices (EPA Standard)
df_aqi = df_aqi.withColumn(
    "aqi",
    greatest(
        col("aqi_pm25"), col("aqi_pm10"), col("aqi_o3"),
        col("aqi_no2"), col("aqi_so2"), col("aqi_co")
    )
)

# Add AQI Category
df_aqi = df_aqi.withColumn(
    "aqi_category",
    when(col("aqi") <= 50, "Good")
    .when((col("aqi") > 50) & (col("aqi") <= 100), "Moderate")
    .when((col("aqi") > 100) & (col("aqi") <= 150), "Unhealthy for Sensitive Groups")
    .when((col("aqi") > 150) & (col("aqi") <= 200), "Unhealthy")
    .when((col("aqi") > 200) & (col("aqi") <= 300), "Very Unhealthy")
    .otherwise("Hazardous")
)

print("   âœ… AQI calculated using EPA formula (max of sub-indices)")

# AQI Distribution
print("\nğŸ“Š AQI Category Distribution:")
print("-"*50)
df_aqi.groupBy("aqi_category").count().orderBy(desc("count")).show()

# AQI Statistics
print("\nğŸ“Š AQI Statistics:")
print("-"*50)
df_aqi.select("aqi").describe().show()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                      SECTION 7: FEATURE ENGINEERING                        â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("âš™ï¸  FEATURE ENGINEERING")
print("="*80)

# 7.1 Parse Timestamp
print("\nğŸ• Extracting Temporal Features...")

df_features = df_aqi.withColumn(
    "timestamp",
    to_timestamp(col("datetime"), "yyyy-MM-dd HH:mm:ss")
)

# Extract time-based features
df_features = df_features.withColumn("hour", hour(col("timestamp")))
df_features = df_features.withColumn("day_of_week", dayofweek(col("timestamp")))
df_features = df_features.withColumn("month", month(col("timestamp")))
df_features = df_features.withColumn("year", year(col("timestamp")))

# Add cyclical encoding for hour (captures circular nature of time)
df_features = df_features.withColumn("hour_sin", expr("sin(2 * 3.14159 * hour / 24)"))
df_features = df_features.withColumn("hour_cos", expr("cos(2 * 3.14159 * hour / 24)"))

print("   âœ… Temporal features extracted: hour, day_of_week, month, year")
print("   âœ… Cyclical time encoding added: hour_sin, hour_cos")

# 7.2 Add Zone/Region Classification (simulated based on data patterns)
print("\nğŸŒ Creating Zone Classification...")

# Create zones based on AQI patterns (simulating country/city since not in data)
df_features = df_features.withColumn(
    "row_id",
    monotonically_increasing_id()
)

# Create synthetic zones based on data patterns (5 zones)
df_features = df_features.withColumn(
    "zone",
    when(col("row_id") % 5 == 0, "Zone_North")
    .when(col("row_id") % 5 == 1, "Zone_South")
    .when(col("row_id") % 5 == 2, "Zone_East")
    .when(col("row_id") % 5 == 3, "Zone_West")
    .otherwise("Zone_Central")
)

print("   âœ… Zone classification created: Zone_North, Zone_South, Zone_East, Zone_West, Zone_Central")

# 7.3 Lag Features (Previous Values for Time Series)
print("\nğŸ“ˆ Creating Lag Features for Time Series...")

# Define window specification
window_spec = Window.partitionBy("zone").orderBy("timestamp")

# Add lag features for key pollutants
df_features = df_features.withColumn("pm25_lag1", lag("pm25", 1).over(window_spec))
df_features = df_features.withColumn("pm25_lag3", lag("pm25", 3).over(window_spec))
df_features = df_features.withColumn("pm10_lag1", lag("pm10", 1).over(window_spec))
df_features = df_features.withColumn("aqi_lag1", lag("aqi", 1).over(window_spec))

# Add rolling statistics (previous 3 hours average)
df_features = df_features.withColumn(
    "pm25_rolling_avg",
    avg("pm25").over(window_spec.rowsBetween(-3, -1))
)

# Fill null lag values with current values
df_features = df_features.fillna({
    "pm25_lag1": df_features.agg(mean("pm25")).collect()[0][0],
    "pm25_lag3": df_features.agg(mean("pm25")).collect()[0][0],
    "pm10_lag1": df_features.agg(mean("pm10")).collect()[0][0],
    "aqi_lag1": df_features.agg(mean("aqi")).collect()[0][0],
    "pm25_rolling_avg": df_features.agg(mean("pm25")).collect()[0][0]
})

print("   âœ… Lag features created: pm25_lag1, pm25_lag3, pm10_lag1, aqi_lag1")
print("   âœ… Rolling statistics: pm25_rolling_avg (3-hour window)")

# 7.4 Weather Interaction Features
print("\nğŸŒ¤ï¸  Creating Weather Interaction Features...")

df_features = df_features.withColumn(
    "temp_humidity_interaction",
    col("temp") * col("rh") / 100
)

df_features = df_features.withColumn(
    "wind_pm25_interaction",
    col("wind") * col("pm25")
)

print("   âœ… Interaction features: temp_humidity_interaction, wind_pm25_interaction")

# Cache the feature-engineered dataframe
df_features = df_features.cache()
print(f"\n   ğŸ“Š Feature-engineered dataset: {df_features.count():,} records")

# Display sample
print("\nğŸ“‹ Sample of Engineered Features:")
print("-"*50)
df_features.select(
    "datetime", "pm25", "aqi", "aqi_category", "zone", 
    "hour", "pm25_lag1", "pm25_rolling_avg"
).show(5, truncate=False)

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                     SECTION 8: ML PIPELINE CONSTRUCTION                    â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ¤– BUILDING ML PIPELINE")
print("="*80)

# 8.1 String Indexing for Categorical Variables
print("\nğŸ”¢ Setting up categorical encoding...")

zone_indexer = StringIndexer(
    inputCol="zone",
    outputCol="zone_index",
    handleInvalid="keep"
)

zone_encoder = OneHotEncoder(
    inputCols=["zone_index"],
    outputCols=["zone_encoded"]
)

print("   âœ… StringIndexer and OneHotEncoder configured for 'zone'")

# 8.2 Feature Assembly
print("\nğŸ“¦ Configuring Feature Assembly...")

# Define feature columns
feature_cols = [
    # Pollutants
    "pm25", "pm10", "no2", "co", "so2", "o3",
    # Weather
    "temp", "rh", "wind", "rain",
    # Temporal
    "hour", "day_of_week", "month", "hour_sin", "hour_cos",
    # Lag features
    "pm25_lag1", "pm25_lag3", "pm10_lag1", "aqi_lag1", "pm25_rolling_avg",
    # Interactions
    "temp_humidity_interaction", "wind_pm25_interaction"
]

# Assembler for numeric features (before encoding)
numeric_assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="numeric_features",
    handleInvalid="skip"
)

# Final assembler (combines numeric with encoded categorical)
final_assembler = VectorAssembler(
    inputCols=["scaled_features", "zone_encoded"],
    outputCol="features",
    handleInvalid="skip"
)

print(f"   âœ… VectorAssembler configured with {len(feature_cols)} numeric features")

# 8.3 Feature Scaling
print("\nğŸ“ Configuring Feature Scaling...")

scaler = StandardScaler(
    inputCol="numeric_features",
    outputCol="scaled_features",
    withStd=True,
    withMean=True
)

print("   âœ… StandardScaler configured (mean=0, std=1)")

# 8.4 Random Forest Regressor
print("\nğŸŒ² Configuring Random Forest Regressor...")

rf_regressor = RandomForestRegressor(
    labelCol="aqi",
    featuresCol="features",
    numTrees=50,
    maxDepth=10,
    minInstancesPerNode=5,
    featureSubsetStrategy="sqrt",
    seed=RANDOM_SEED
)

print("   âœ… RandomForestRegressor: numTrees=50, maxDepth=10")

# 8.5 Build Complete Pipeline
print("\nğŸ”— Assembling Complete Pipeline...")

pipeline = Pipeline(stages=[
    zone_indexer,      # Step 1: Index categorical zone
    zone_encoder,      # Step 2: One-hot encode zone
    numeric_assembler, # Step 3: Assemble numeric features
    scaler,            # Step 4: Scale numeric features
    final_assembler,   # Step 5: Combine all features
    rf_regressor       # Step 6: Random Forest model
])

print("   âœ… Pipeline assembled with 6 stages")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                      SECTION 9: MODEL TRAINING & EVALUATION                â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ¯ SECTION 9: MODEL TRAINING & SPARK EXECUTION MODEL")
print("="*80)

print("""
ğŸ“š SPARK EXECUTION MODEL - DAG & STAGES:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  When you submit a Spark job:                                               â”‚
â”‚                                                                             â”‚
â”‚  1. DAG (Directed Acyclic Graph) Creation:                                  â”‚
â”‚     â€¢ Spark builds a DAG of operations from your code                       â”‚
â”‚     â€¢ Transformations are nodes, dependencies are edges                     â”‚
â”‚                                                                             â”‚
â”‚  2. DAG Scheduler:                                                          â”‚
â”‚     â€¢ Divides DAG into STAGES at shuffle boundaries                        â”‚
â”‚     â€¢ Each stage contains tasks that can run in parallel                   â”‚
â”‚                                                                             â”‚
â”‚  3. Task Scheduler:                                                         â”‚
â”‚     â€¢ Schedules TASKS (one per partition) to executors                     â”‚
â”‚     â€¢ Tasks within a stage can run in parallel                             â”‚
â”‚                                                                             â”‚
â”‚  4. Executors:                                                              â”‚
â”‚     â€¢ Run tasks on worker nodes                                            â”‚
â”‚     â€¢ Store data in memory/disk (caching)                                  â”‚
â”‚                                                                             â”‚
â”‚  NARROW vs WIDE Transformations:                                            â”‚
â”‚  â€¢ NARROW (no shuffle): map, filter, union - one partition to one          â”‚
â”‚  â€¢ WIDE (shuffle): groupByKey, reduceByKey, join - many to many            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# 9.1 Train/Test Split
print("\nâœ‚ï¸  Splitting Data (80/20)...")

# Filter out any remaining null values in critical columns
ml_data = df_features.select(
    feature_cols + ["aqi", "zone", "datetime", "aqi_category"]
).na.drop()

train_data, test_data = ml_data.randomSplit([TRAIN_RATIO, TEST_RATIO], seed=RANDOM_SEED)

# Cache training and test data
train_data = train_data.cache()
test_data = test_data.cache()

train_count = train_data.count()
test_count = test_data.count()

print(f"   ğŸ“Š Training set: {train_count:,} records ({TRAIN_RATIO*100:.0f}%)")
print(f"   ğŸ“Š Test set: {test_count:,} records ({TEST_RATIO*100:.0f}%)")

# 9.2 Demonstrate Execution Plan (explains query optimization)
print("\nğŸ“‹ EXPLAIN EXECUTION PLAN:")
print("-"*60)
print("   (Shows how Spark will execute the query)")
print("")
train_data.select("pm25", "pm10", "aqi", "zone").explain(mode="simple")

# 9.3 Show DAG information
print("\nğŸ“Š DAG (Directed Acyclic Graph) Information:")
print("-"*60)
print(f"   RDD Lineage (Debug String):")
print(f"   {train_data.rdd.toDebugString().decode('utf-8')[:500]}...")

# 9.4 Train Model
print("\nğŸ‹ï¸  Training Random Forest Model...")
print("   â³ This may take a few minutes...")

print("""
   ğŸ“š ML Pipeline Stages:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  Stage 1: StringIndexer    â†’ Convert zone to numeric index â”‚
   â”‚  Stage 2: OneHotEncoder    â†’ One-hot encode zone           â”‚
   â”‚  Stage 3: VectorAssembler  â†’ Combine numeric features      â”‚
   â”‚  Stage 4: StandardScaler   â†’ Normalize features            â”‚
   â”‚  Stage 5: VectorAssembler  â†’ Combine all features          â”‚
   â”‚  Stage 6: RandomForest     â†’ Train regression model        â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

try:
    start_time = datetime.now()
    
    # Fit the pipeline - This triggers the DAG execution
    model = pipeline.fit(train_data)
    
    training_time = (datetime.now() - start_time).total_seconds()
    print(f"   âœ… Model trained successfully in {training_time:.2f} seconds!")
    
    # Show pipeline stages
    print(f"\n   ğŸ“Š Pipeline Stages Executed: {len(model.stages)}")
    for i, stage in enumerate(model.stages):
        print(f"       Stage {i+1}: {type(stage).__name__}")
    
except Exception as e:
    print(f"   âŒ Training failed: {e}")
    spark.stop()
    sys.exit(1)

# 9.3 Make Predictions
print("\nğŸ”® Making Predictions on Test Set...")

predictions = model.transform(test_data)
predictions = predictions.cache()

print("   âœ… Predictions generated!")

# Display sample predictions
print("\nğŸ“‹ Sample Predictions:")
print("-"*80)
predictions.select(
    "datetime", "zone", "pm25", "aqi", "prediction", "aqi_category"
).show(10, truncate=False)

# 9.4 Model Evaluation
print("\n" + "="*80)
print("ğŸ“Š MODEL EVALUATION METRICS")
print("="*80)

# Initialize evaluators
rmse_evaluator = RegressionEvaluator(
    labelCol="aqi", 
    predictionCol="prediction", 
    metricName="rmse"
)

mae_evaluator = RegressionEvaluator(
    labelCol="aqi", 
    predictionCol="prediction", 
    metricName="mae"
)

r2_evaluator = RegressionEvaluator(
    labelCol="aqi", 
    predictionCol="prediction", 
    metricName="r2"
)

# Calculate metrics
rmse = rmse_evaluator.evaluate(predictions)
mae = mae_evaluator.evaluate(predictions)
r2 = r2_evaluator.evaluate(predictions)

print(f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        MODEL PERFORMANCE METRICS                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  ğŸ“ˆ Root Mean Square Error (RMSE):  {rmse:>10.4f}                              â•‘
â•‘  ğŸ“ˆ Mean Absolute Error (MAE):      {mae:>10.4f}                              â•‘
â•‘  ğŸ“ˆ R-Squared (RÂ²):                 {r2:>10.4f}                              â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  Interpretation:                                                              â•‘
â•‘  â€¢ RMSE: Average prediction error in AQI units                               â•‘
â•‘  â€¢ MAE: Average absolute prediction error                                    â•‘
â•‘  â€¢ RÂ²: {r2*100:.1f}% of variance explained by the model                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

# 9.5 Prediction Error Analysis
print("\nğŸ“Š Prediction Error Analysis:")
print("-"*50)

predictions_with_error = predictions.withColumn(
    "prediction_error",
    col("aqi") - col("prediction")
).withColumn(
    "abs_error",
    expr("abs(prediction_error)")
)

predictions_with_error.select("prediction_error", "abs_error").describe().show()

# Error distribution by AQI category
print("\nğŸ“Š Mean Absolute Error by AQI Category:")
print("-"*50)
predictions_with_error.groupBy("aqi_category").agg(
    spark_round(avg("abs_error"), 2).alias("mean_abs_error"),
    count("*").alias("count")
).orderBy("mean_abs_error").show()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                      SECTION 10: FUTURE FORECASTING                        â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ”® FUTURE AQI FORECASTING")
print("="*80)

# 10.1 Create Future DataFrame
print("\nğŸ“… Creating Future Forecast Dataset (Next 24 Hours)...")

# Get average values for features from the dataset
avg_features = df_features.agg({
    'pm25': 'mean', 'pm10': 'mean', 'no2': 'mean', 'co': 'mean',
    'so2': 'mean', 'o3': 'mean', 'temp': 'mean', 'rh': 'mean',
    'wind': 'mean', 'rain': 'mean'
}).collect()[0]

# Create future data using Spark-native approach (avoids Python worker issues)
# Take a sample row and modify it for forecasting
print("   ğŸ“Š Using average values for forecast baseline...")

# Get average values
avg_pm25 = float(avg_features['avg(pm25)'])
avg_pm10 = float(avg_features['avg(pm10)'])
avg_no2 = float(avg_features['avg(no2)'])
avg_co = float(avg_features['avg(co)'])
avg_so2 = float(avg_features['avg(so2)'])
avg_o3 = float(avg_features['avg(o3)'])
avg_temp = float(avg_features['avg(temp)'])
avg_rh = float(avg_features['avg(rh)'])
avg_wind = float(avg_features['avg(wind)'])
avg_rain = float(avg_features['avg(rain)'])

# Create future DataFrame using SQL literals (no Python UDFs needed)
future_rows = []
for i in range(24):
    future_rows.append((
        i,  # hour
        avg_pm25, avg_pm10, avg_no2, avg_co, avg_so2, avg_o3,
        avg_temp, avg_rh, avg_wind, avg_rain,
        (i % 7) + 1,  # day_of_week
        12,  # month (December)
        "Zone_Central",
        avg_pm25, avg_pm25, avg_pm10, 100.0, avg_pm25,
        avg_temp * avg_rh / 100,
        avg_wind * avg_pm25,
        0.0, "Unknown"
    ))

future_schema = StructType([
    StructField("hour", IntegerType(), True),
    StructField("pm25", DoubleType(), True),
    StructField("pm10", DoubleType(), True),
    StructField("no2", DoubleType(), True),
    StructField("co", DoubleType(), True),
    StructField("so2", DoubleType(), True),
    StructField("o3", DoubleType(), True),
    StructField("temp", DoubleType(), True),
    StructField("rh", DoubleType(), True),
    StructField("wind", DoubleType(), True),
    StructField("rain", DoubleType(), True),
    StructField("day_of_week", IntegerType(), True),
    StructField("month", IntegerType(), True),
    StructField("zone", StringType(), True),
    StructField("pm25_lag1", DoubleType(), True),
    StructField("pm25_lag3", DoubleType(), True),
    StructField("pm10_lag1", DoubleType(), True),
    StructField("aqi_lag1", DoubleType(), True),
    StructField("pm25_rolling_avg", DoubleType(), True),
    StructField("temp_humidity_interaction", DoubleType(), True),
    StructField("wind_pm25_interaction", DoubleType(), True),
    StructField("aqi", DoubleType(), True),
    StructField("aqi_category", StringType(), True)
])

future_df = spark.createDataFrame(future_rows, future_schema)

# Add cyclical hour encoding using Spark SQL
future_df = future_df.withColumn("hour_sin", expr("sin(2 * 3.14159 * hour / 24)"))
future_df = future_df.withColumn("hour_cos", expr("cos(2 * 3.14159 * hour / 24)"))

print(f"   âœ… Created forecast dataset with 24 hourly predictions")

# 10.2 Generate Forecasts
print("\nğŸ”® Generating AQI Forecasts...")

try:
    forecast = model.transform(future_df)
    
    # Add predicted category
    forecast = forecast.withColumn(
        "predicted_category",
        when(col("prediction") <= 50, "Good")
        .when((col("prediction") > 50) & (col("prediction") <= 100), "Moderate")
        .when((col("prediction") > 100) & (col("prediction") <= 150), "Unhealthy for Sensitive Groups")
        .when((col("prediction") > 150) & (col("prediction") <= 200), "Unhealthy")
        .when((col("prediction") > 200) & (col("prediction") <= 300), "Very Unhealthy")
        .otherwise("Hazardous")
    )
    
    print("\nğŸ“… 24-Hour AQI Forecast:")
    print("-"*80)
    forecast.select(
        "hour", "zone", 
        spark_round("prediction", 1).alias("predicted_aqi"),
        "predicted_category"
    ).orderBy("hour").show(24, truncate=False)
    
except Exception as e:
    print(f"   âš ï¸  Forecasting warning: {e}")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                      SECTION 11: HIGH-RISK ZONE ANALYSIS                   â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("âš ï¸  HIGH-RISK ZONE IDENTIFICATION")
print("="*80)

# 11.1 Analyze Predictions by Zone
print("\nğŸŒ Analyzing Pollution Levels by Zone...")

zone_analysis = predictions.groupBy("zone").agg(
    spark_round(avg("prediction"), 2).alias("avg_predicted_aqi"),
    spark_round(spark_max("prediction"), 2).alias("max_predicted_aqi"),
    spark_round(spark_min("prediction"), 2).alias("min_predicted_aqi"),
    spark_round(stddev("prediction"), 2).alias("std_predicted_aqi"),
    count("*").alias("num_records")
).orderBy(desc("avg_predicted_aqi"))

print("\nğŸ“Š Zone-wise AQI Analysis (All Zones):")
print("-"*80)
zone_analysis.show()

# 11.2 Identify High-Risk Zones (AQI > 150)
print("\nğŸš¨ Identifying High-Risk Zones (Average AQI > 150)...")

HIGH_RISK_THRESHOLD = 150

high_risk_zones = zone_analysis.filter(
    col("avg_predicted_aqi") > HIGH_RISK_THRESHOLD
).orderBy(desc("avg_predicted_aqi"))

high_risk_count = high_risk_zones.count()

if high_risk_count > 0:
    print(f"\nâš ï¸  Found {high_risk_count} HIGH-RISK ZONES:")
    print("-"*80)
    high_risk_zones.show()
else:
    print(f"\nâœ… No zones exceed the high-risk threshold (AQI > {HIGH_RISK_THRESHOLD})")
    print("   Showing top 5 zones with highest average predicted AQI:")
    zone_analysis.limit(5).show()

# 11.3 Time-based Risk Analysis
print("\nğŸ• Risk Analysis by Time of Day:")
print("-"*80)

hourly_risk = predictions.groupBy("hour").agg(
    spark_round(avg("prediction"), 2).alias("avg_predicted_aqi"),
    spark_round(spark_max("prediction"), 2).alias("max_aqi")
).orderBy("hour")

hourly_risk.show(24)

# Find peak pollution hours
print("\nâ° Peak Pollution Hours (Top 5):")
print("-"*80)
predictions.groupBy("hour").agg(
    spark_round(avg("prediction"), 2).alias("avg_aqi")
).orderBy(desc("avg_aqi")).limit(5).show()

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                         SECTION 12: SAVE RESULTS                           â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ’¾ SAVING RESULTS")
print("="*80)

# 12.1 Save Predictions using Pandas (Windows-compatible, no Hadoop needed)
print("\nğŸ“ Saving Predictions...")

try:
    predictions_output = predictions.select(
        "datetime", "zone", "pm25", "pm10", "no2", "co", "so2", "o3",
        "temp", "rh", "wind", "aqi", "prediction", "aqi_category"
    ).withColumn(
        "prediction", spark_round("prediction", 2)
    )
    
    # Convert to Pandas and save (Windows-friendly approach)
    predictions_pdf = predictions_output.limit(10000).toPandas()  # Limit for memory
    predictions_pdf.to_csv(PREDICTIONS_OUTPUT, index=False)
    
    print(f"   âœ… Predictions saved to: {PREDICTIONS_OUTPUT}")
    print(f"   ğŸ“Š Saved {len(predictions_pdf):,} sample predictions")
    
except Exception as e:
    print(f"   âš ï¸  Could not save predictions: {e}")

# 12.2 Save High-Risk Zones
print("\nğŸ“ Saving High-Risk Zones Analysis...")

try:
    # Save all zones with risk classification
    zones_with_risk = zone_analysis.withColumn(
        "risk_level",
        when(col("avg_predicted_aqi") > 200, "CRITICAL")
        .when(col("avg_predicted_aqi") > 150, "HIGH")
        .when(col("avg_predicted_aqi") > 100, "MODERATE")
        .when(col("avg_predicted_aqi") > 50, "LOW")
        .otherwise("MINIMAL")
    )
    
    # Convert to Pandas and save (Windows-friendly approach)
    zones_pdf = zones_with_risk.toPandas()
    zones_pdf.to_csv(HIGH_RISK_OUTPUT, index=False)
    
    print(f"   âœ… High-risk zones saved to: {HIGH_RISK_OUTPUT}")
    
except Exception as e:
    print(f"   âš ï¸  Could not save high-risk zones: {e}")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                        SECTION 13: FEATURE IMPORTANCE                      â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ” FEATURE IMPORTANCE ANALYSIS")
print("="*80)

try:
    # Extract Random Forest model from pipeline
    rf_model = model.stages[-1]
    
    # Get feature importances
    importances = rf_model.featureImportances.toArray()
    
    # Create feature names list (numeric features + encoded features)
    feature_names = feature_cols + ["zone_encoded"]
    
    print("\nğŸ“Š Top 10 Most Important Features:")
    print("-"*50)
    
    # Sort and display top features
    importance_pairs = list(zip(feature_names[:len(importances)], importances))
    importance_pairs.sort(key=lambda x: x[1], reverse=True)
    
    for i, (feature, importance) in enumerate(importance_pairs[:10], 1):
        bar = "â–ˆ" * int(importance * 50)
        print(f"   {i:2}. {feature:30} {importance:.4f} {bar}")
        
except Exception as e:
    print(f"   âš ï¸  Could not extract feature importances: {e}")

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                          SECTION 14: FINAL SUMMARY                         â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                         ğŸ¯ PROJECT SUMMARY ğŸ¯                                 â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  âœ… SPARK ARCHITECTURE DEMONSTRATED:                                         â•‘
â•‘     â€¢ Driver Program: Main script coordinating execution                    â•‘
â•‘     â€¢ SparkContext (sc): Low-level RDD API entry point                      â•‘
â•‘     â€¢ SparkSession (spark): Unified DataFrame/SQL entry point               â•‘
â•‘     â€¢ Executors: Worker processes running tasks                             â•‘
â•‘     â€¢ Partitions: Data divided across nodes ({1} partitions used)           â•‘
â•‘                                                                              â•‘
â•‘  âœ… RDD OPERATIONS PERFORMED:                                                â•‘
â•‘     â€¢ Transformations: map, filter, reduceByKey, groupByKey, flatMap,       â•‘
â•‘                       mapPartitions, sortByKey, union, distinct             â•‘
â•‘     â€¢ Actions: collect, count, reduce, take, first, countByKey              â•‘
â•‘     â€¢ Shared Variables: Accumulators, Broadcast variables                   â•‘
â•‘                                                                              â•‘
â•‘  âœ… DATAFRAME & SPARK SQL:                                                   â•‘
â•‘     â€¢ DataFrame API for structured data processing                          â•‘
â•‘     â€¢ SQL queries via createOrReplaceTempView()                             â•‘
â•‘     â€¢ Window functions for rolling calculations                             â•‘
â•‘     â€¢ Catalyst optimizer for query optimization                             â•‘
â•‘                                                                              â•‘
â•‘  âœ… DATA PROCESSING:                                                         â•‘
â•‘     â€¢ Loaded and cached {0:,} records                                   â•‘
â•‘     â€¢ Handled missing values with mean imputation                            â•‘
â•‘     â€¢ Filtered invalid readings                                              â•‘
â•‘                                                                              â•‘
â•‘  âœ… AQI CALCULATION:                                                         â•‘
â•‘     â€¢ Implemented EPA standard AQI formula                                   â•‘
â•‘     â€¢ Calculated sub-indices for PM2.5, PM10, O3, NO2, SO2, CO              â•‘
â•‘     â€¢ Overall AQI = max of all sub-indices                                   â•‘
â•‘                                                                              â•‘
â•‘  âœ… FEATURE ENGINEERING:                                                     â•‘
â•‘     â€¢ Temporal features: hour, day_of_week, month, cyclical encoding        â•‘
â•‘     â€¢ Lag features: pm25_lag1, pm25_lag3, pm10_lag1, aqi_lag1               â•‘
â•‘     â€¢ Rolling statistics: 3-hour moving average                              â•‘
â•‘     â€¢ Interaction features: tempÃ—humidity, windÃ—pm25                         â•‘
â•‘                                                                              â•‘
â•‘  âœ… ML PIPELINE (DAG Execution):                                             â•‘
â•‘     â€¢ StringIndexer â†’ OneHotEncoder â†’ StandardScaler                        â•‘
â•‘     â€¢ VectorAssembler â†’ RandomForestRegressor (50 trees)                    â•‘
â•‘     â€¢ DAG scheduling with narrow/wide transformations                       â•‘
â•‘                                                                              â•‘
â•‘  âœ… MODEL PERFORMANCE:                                                       â•‘
â•‘     â€¢ RMSE: {2:.4f}                                                          â•‘
â•‘     â€¢ MAE:  {3:.4f}                                                          â•‘
â•‘     â€¢ RÂ²:   {4:.4f} ({5:.1f}% variance explained)                            â•‘
â•‘                                                                              â•‘
â•‘  âœ… OUTPUTS GENERATED:                                                       â•‘
â•‘     â€¢ aqi_predictions_output.csv - Model predictions                        â•‘
â•‘     â€¢ aqi_high_risk_zones.csv - Zone-level risk analysis                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""".format(record_count, NUM_PARTITIONS, rmse, mae, r2, r2*100))

# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
# â•‘                      SECTION 15: CLEANUP & SHUTDOWN                        â•‘
# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("\n" + "="*80)
print("ğŸ›‘ SECTION 15: SPARK SESSION CLEANUP & SHUTDOWN")
print("="*80)

print("""
ğŸ“š CLEANUP BEST PRACTICES:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. UNPERSIST cached RDDs/DataFrames to free memory                        â”‚
â”‚  2. STOP SparkContext/SparkSession to release resources                    â”‚
â”‚  3. Close any open connections (HDFS, JDBC, etc.)                          â”‚
â”‚                                                                             â”‚
â”‚  Memory Management:                                                          â”‚
â”‚  â€¢ Driver Memory: Used for collecting results, broadcast variables         â”‚
â”‚  â€¢ Executor Memory: Used for caching, shuffle, task execution              â”‚
â”‚  â€¢ Storage Memory: For persisting RDDs                                      â”‚
â”‚  â€¢ Execution Memory: For shuffles, joins, sorts                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
""")

# Print final Spark statistics
print("ğŸ“Š Final Spark Statistics:")
print("-"*60)
print(f"   â€¢ Application ID: {sc.applicationId}")
print(f"   â€¢ Application Name: {sc.appName}")
print(f"   â€¢ Spark Version: {spark.version}")
print(f"   â€¢ Default Parallelism: {sc.defaultParallelism}")

# Unpersist cached DataFrames
print("\nğŸ’¾ Unpersisting cached DataFrames...")
try:
    df_raw.unpersist()
    print("   âœ… df_raw unpersisted")
except:
    pass

try:
    df_features.unpersist()
    print("   âœ… df_features unpersisted")
except:
    pass

try:
    predictions.unpersist()
    print("   âœ… predictions unpersisted")
except:
    pass

try:
    train_data.unpersist()
    test_data.unpersist()
    print("   âœ… train_data and test_data unpersisted")
except:
    pass

# Stop SparkContext and SparkSession
print("\nğŸ›‘ Stopping Spark Session and Context...")
spark.stop()

print("""
âœ… Spark Session stopped successfully!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    SPARK CONCEPTS DEMONSTRATED SUMMARY                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘                                                                              â•‘
â•‘  DRIVER PROGRAM CONCEPTS:                                                    â•‘
â•‘    âœ“ SparkContext creation and configuration                                â•‘
â•‘    âœ“ SparkSession as unified entry point                                    â•‘
â•‘    âœ“ Application coordination and DAG scheduling                            â•‘
â•‘    âœ“ Collecting results back to driver                                      â•‘
â•‘                                                                              â•‘
â•‘  RDD OPERATIONS:                                                             â•‘
â•‘    âœ“ Transformations: map, filter, flatMap, reduceByKey, groupByKey,        â•‘
â•‘                      mapPartitions, sortByKey, distinct                     â•‘
â•‘    âœ“ Actions: collect, count, reduce, take, first, countByKey,              â•‘
â•‘              saveAsTextFile, foreach                                        â•‘
â•‘    âœ“ Pair RDD operations: reduceByKey, groupByKey, sortByKey, join          â•‘
â•‘                                                                              â•‘
â•‘  SHARED VARIABLES:                                                           â•‘
â•‘    âœ“ Broadcast Variables: Shared read-only data to all nodes                â•‘
â•‘    âœ“ Accumulators: Shared write-only counters/sums                          â•‘
â•‘                                                                              â•‘
â•‘  DATAFRAME & SQL:                                                            â•‘
â•‘    âœ“ DataFrame API for structured data                                      â•‘
â•‘    âœ“ Spark SQL with createOrReplaceTempView                                 â•‘
â•‘    âœ“ Window functions and aggregations                                      â•‘
â•‘    âœ“ Catalyst optimizer (explain plans)                                     â•‘
â•‘                                                                              â•‘
â•‘  EXECUTION MODEL:                                                            â•‘
â•‘    âœ“ DAG (Directed Acyclic Graph) creation                                  â•‘
â•‘    âœ“ Stage division at shuffle boundaries                                   â•‘
â•‘    âœ“ Task scheduling and execution                                          â•‘
â•‘    âœ“ Narrow vs Wide transformations                                         â•‘
â•‘                                                                              â•‘
â•‘  MEMORY & CACHING:                                                           â•‘
â•‘    âœ“ cache() and persist() for RDDs/DataFrames                              â•‘
â•‘    âœ“ Storage levels (MEMORY_ONLY, MEMORY_AND_DISK)                          â•‘
â•‘    âœ“ Partitioning strategies                                                â•‘
â•‘    âœ“ unpersist() for cleanup                                                â•‘
â•‘                                                                              â•‘
â•‘  ML PIPELINE:                                                                â•‘
â•‘    âœ“ Transformers and Estimators                                            â•‘
â•‘    âœ“ Pipeline stages and fitting                                            â•‘
â•‘    âœ“ Model persistence and evaluation                                       â•‘
â•‘                                                                              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
""")

print("\n" + "="*80)
print("ğŸ‰ AIR QUALITY PREDICTION PROJECT COMPLETED SUCCESSFULLY! ğŸ‰")
print("="*80)
print("""
Output Files Generated:
  ğŸ“„ aqi_predictions_output.csv - Model predictions
  ğŸ“„ aqi_high_risk_zones.csv   - Zone risk analysis

All Spark concepts (Driver, SparkContext, RDD, Transformations, 
Actions, Accumulators, Broadcast, DAG, Stages) have been demonstrated!
""")
print("="*80 + "\n")
